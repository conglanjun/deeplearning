# 第五章 机器学习基础
深度学习是机器学习的一个特定分支。要充分理解深度学习，必须对机器学习基本原理有深刻理解。大部分机器学习算法都有`超参数`(必须在学习算法外设定)。机器学习本质上属于应用统计学，更多的关注于如何用计算机统计的估计复杂函数，不太关注为这些函数提供置信区间；因此我们会探讨两种统计学的主要方法：频率派估计和贝叶斯推断。大部分机器学习算法可以可以分成监督学习和无监督学习两类；我们将探讨不同的分类。
## 5.1 学习算法
机器学习算法是一种能够从数据中学习的算法。所谓的“学习”简洁的定义：“对于某类任务T和性能度量P，一个计算程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。”经验E，任务T和性能度量P定义范围非常宽广。
### 5.1.1 任务T
机器学习可以让我们解决一些认为设计和使用确定性程序很难解决的问题。
从“任务”相对正式的定义上说，学习过程本身不能算是任务。学习是我们所谓的获取完成任务的能力。例如我们目标使机器人行走，那么行走便是任务。
通常机器学习任务定义为机器学习系统应该如何处理`样本`(example)。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的`特征(feature)`的集合。通常将样本表示成向量$x\in R^n$，其中向量的每一个元素$x_i$是一个特征。例如一张图片的特征通常是指这张图片的像素值。
机器学习可以解决很多类型的任务。常见机器学习任务列举如下：
- 分类：在这类任务中，计算机程序需要指定某些输入属于k类中的哪一类。为了完成这个任务，学习算法通常会返回一个函数$f: \mathbb{R}^n\rightarrow \{1,...,k\}$。当$y=f(x)$时，模型将向量x所代表的输入分类到数字编码y所代表的类别。还有一些其他分类问题，例如，f输出的事不同类别的概率分布。分类任务中有一个任务是对象识别，其中输入是图片(通常由一组像素亮度值表示)，输出是表示图片物体的数字码。例如，Willow Garage PR2机器人能像服务员一样识别不同饮料，并送给点餐顾客。目前，最好的对象识别工具正是基于深度学习的。对象识别同时也是计算机识别人脸的基础技术，可用于标记相片合辑中的人脸，有助于计算机更自然的与用户交互。
- 输入缺失分类：当输入向量每一个度量不被保证的时候，分类问题讲会变得更有挑战性。为了解决分类任务，学习算法只需要定义一个从输入向量映射到输出类别的函数。当一些输入可能丢失时，学习算法必须学习一组函数，而不是单个分类函数。而是让一系列函数学习，每个函数对应着分类具有不同缺失输入子集的x。(个人理解：对缺失维度进行单独训练一个适合的模型，为适应缺失维度类型的输入数据。当输入数据不缺失时，用全维度模型训练)。医疗诊断中经常出现，有些医学测试昂贵，对身体有害。有效定义这样一个大集合函数的方法是学习所有相关变量概率分布，然后通过边缘化缺失变量来解决分类任务。使用n个输入变量(个人理解成n维)，我们现在可以获得每个可能的缺失输入集合所需的所有$2^n$个不同的分类函数，但是计算机程序仅需要学习一个描述联合概率分布的函数。参见Goodfellow (2013d)了解以这种方式将深度概率模型应用于这类任务的示例。本节中描述的许多其他任务也可以推广到缺失输入的情况；确实输入分类只是机器学习能够解决的问题的一个示例。
- 回归：在这类任务中，计算机程序需要对给定输入预测数值。为了解决这个任务，学习算法需要输出函数$f:\mathbb{R}^n\rightarrow \mathbb{R}$。除了返回结果的形式不一样外，这类问题和分类问题是很像的。这类任务的一个示例是预测投保人的索赔金额(用于设置险费)，或者预测证券未来价格。这类也用在算法交易中。
- 转录：这类任务中，机器学习系统观测一些相对非结构化的表示数据，并转录信息为离散的文本形式。例如，光学字符识别要求计算机程序根据文本图片返回文字序列(ASCII码或者Unicode码)。谷歌街景以这种方式使用深度学习处理街道编号(Goodfellow 2014d)。另一个例子是语音识别，计算机程序输入一段音频波形，输出一序列音频记录中所说的字符或单词ID的编码。深度学习是现代语音识别系统重要组成部分，被各大公司广泛使用，包括微软，IBM和谷歌。
- 机器翻译：在机器翻译任务中，输入是一种语言的符号序列，计算机程序必须将其转化成另一种语言的符号序列。这通常适用于自然语言，如将英语翻译成法语。最近深度学习已经开始在这个任务上产生重要影响。
- 结构化输出：结构化输出任务的输出是向量或者其他包含多个值的数据结构，并且构成输出的不同元素间具有重要关系。这是一个很大的范畴，包括上述转录任务和翻译任务在内的很多其他任务。例如语法分析--映射自然语言句子到语法结构树，并标记树的节点为动词、名词、副词等等。参考Collobert(2011)将深度学习应用到语法分析的示例。另一个例子是图像的像素级分割，将每一个像素分配到特定类别。例如，深度学习可用于标注航拍图片中的道路位置。在这类标注型任务中，输出的结构形式不需要和输入尽可能相似。例如，在为图片添加描述的任务中，计算机程序观察到一幅图输出描述这幅图的自然语言句子。这类任务被称为`结构化输出任务`是因为输出值之间内部紧密相关。例如，为图片添加标题的程序输出的单词必须合成一个通顺的句子。
- 异常检测：在这类任务中，计算机程序在一组事件或对象中筛选，并标记不正常或非典型的个体。异常检测任务的一个示例是信用卡欺诈检测。通过对你的购买习惯建模，信用卡公司可以检测到你的卡是否被滥用。如果窃贼窃取你的信用卡或信用卡信息，窃贼采购物品的分布通常和你的不同。党该卡发生了不正常的购买行为时，信用卡公司可以尽快冻结该卡以防欺诈。参考Chandola了解欺诈检测方法。
- 合成和采样：在这类任务中，机器学习生成一些和训练数据相似的新样本。通过机器学习，合成和采样可能在媒体应用中非常有用，可以避免艺术家大量昂贵或者乏味费时的手动工作。例如，视频游戏可以自动生成大型物体或风景的纹理，而不是让艺术家手动标记每个像素。在某些情况下，我们希望采样或合成过程可以根据给定的输入生成一些特定类型得输出。例如，在语音合成任务中，我们提供书写的句子，要求程序输出这个句子的语音音频波形。这是一类节结构化输出任务，并且要求每个输入并非只有一个正确的输出，并且希望输出有很多变化。这可以使结果看上去更加自然和真实。
- 缺失值填补：在这类任务中，机器学习算法给定一个新样本$x\in \mathbb{R}^n$，x中某些元素$x_i$缺失。算法必须填补这些缺失值。
- 去噪：在这类任务中，机器学习算法输入是干净样本$x\in \mathbb{R}^n$经过未知损坏过程后得到的损坏样本$\tilde{x}\in \mathbb{R}^n$，算法根据损坏后样本$\tilde{x}$预测干净的样本x，或者更一般的预测条件概率分布$p(x|\tilde{x})$。
- 密度估计或概率质量函数估计：在密度估计问题中，机器学习算法学习函数$p_{model}: \mathbb{R}\rightarrow\mathbb{R}$，其中$p_{model}(x)$可以解释成样本采样空间的概率密度函数(如果x是连续的)或者概率质量函数(如果x是离散的)。要做好这样的任务，算法要学习观测到的数据结构。算法必须知道什么情况下样本聚集出现，什么情况下不太可能出现。以上描述的大多数任务都要求学习算法至少能隐式地捕获概率分布的结构。密度估计可以让我们显式捕获该分布。原则上，我们可以在该分布上计算以便解决其他任务。例如，如果我们通过密度估计得到了概率分布$p(x)$，我们可以用该分布解决缺失值填补任务。如果$x_i$的值是缺失的，但是其他的变量值$x_{-i}$已知，那么我们可以得到概率分布$p(x_i|x_{-i})$。实际情况中，密度估计并不能够解决所有这类问题，因为在很多情况下p(x)是难以计算的。
### 5.1.2 性能度量P
为了评估机器学习算法的能力，我们必须设计其性能的定量度量。通常性能度量P是特定于系统执行的任务T而言。
对于诸如分类、缺失输入分类和转录任务，我们通常度量模型的`准确率(accuracy)`。准确率是指该模型输出正确结果的样本比率。我们也可以通过`错误率(error rate)`得到相同的信息。错误率是指该模型输出错误结果的样本比率。通常把错误率称为$0-1$损失的期望。在一个特定样本上，如果结果是对的那么$0-1$损失是0；否则是1。但是对于密度估计这类任务而言，度量准确率、错误率或者其他类型的$0-1$损失是没有意义的。反之，我们必须使用不同的性能度量，使模型对每个样本都输出一个连续数值的得分。最常用的方法是输出模型在一些样本上概率对数的平均值。
通常，我们会更关注机器学习算法在未观测数据上的性能如何，因为这将决定其在实际应用中的性能。因此，我们使用`测试集(test set)`数据来评估系统性能，将其与训练机器学习系统的训练集数据分开。
性能度量的选择或许看上去简单且客观，但是选择一个与系统理想表现对应的性能度量通常是很难的。
某些情况下很难确定度量是什么。例如，在执行转录任务时我们是应该度量系统转录整个序列的准确率，还是用一个更加细粒度的指标，对序列中正确的部分元素以正面评价？在执行回归任务时，我们应该更多的惩罚频繁犯一些中等错误的系统，还是较少犯错但是犯很大错误的系统？这些设计的选择取决于应用。
还有些情况，我们知道应该度量哪些数值，但是度量他们不太现实。这种情况经常出现在密度估计中。很多最好的概率模型只能隐式的表示概率分布。在许多这类模型中，计算空间中特定点的人概率是不可能的，在这些情况下，我们必须设计仍然对应于设计对象的替代标准，或者设计一个理想标准的良好近似。
### 5.1.3 经验E
根据学习过程中的不同经验，机器学习算法可以大致分为`无监督(unsupervised)`算法和`监督(supervised)`算法。
本书中大部分算法可以被理解为在整个`数据集(dataset)`获取经验。数据库是指很多样本组成的集合，有时也将样本称为`数据点(data point)`。
Iris(鸢尾花卉)数据集，是统计学家和机器学习研究者使用了很久的数据集。它是150个鸢尾花卉植物不同部分测量结果的集合。每个单独的植物对应一个样本。每个样本的特征是该植物不同部分的测量结果：萼片长度、萼片宽度、花瓣长度和花瓣宽度。这个数据集也记录了每个植物属于什么品种，其中共有三个不同品种。
`无监督学习算法(unsupervised learning algorithm)`训练含有特征的数据集，然后学习出这个数据集上有用的结构性质。在深度学习中，我们通过要学习生成数据集的整个概率分布，显式的，比如密度估计，或是隐式的，比如合成和去噪。还有一些其他类型的无监督学习任务，例如聚类，将数据集分成相似样本的集合。
`监督学习算法(supervised learning algorithm)`训练含有很多特征的数据集，不过数据集中的样本都有一个`标签(label)`或`目标(target)`。例如，Iris数据集证明了每个鸢尾花卉样本属于什么品种。监督学习算法通过学习Iris数据集，学习如何根据测量结果将样本划分为三个不同品种。
大致来说，无监督学习涉及到观察随机向量x的好几个样本，试图显式或隐式地学习出概率分布$p(x)$，或者是该分布一些有意思的性质；而监督学习包含观察随机向量x及其相关联的值或向量y，然后从x预测y，通常是估计$p(y|x)$。术语`监督学习(supervised learning)`源于这样一个视角，教员或老师提供目标y给机器学习系统，指导其应该做什么。在无监督学习中，没有教员或者老师，算法必须学会在没有指导的人情况下理解数据。
无监督学习和监督学习不是严格意义的术语。界限很模糊。很多机器学习技术可以用于这两个任务。例如，概率的链式法则表明对于向量$x\in\mathbb{R}^n$，联合分布可以分解成
$$
p(x)=\prod_{i=1}^np(x_i|x_1,...,x_{i-1})
$$
该分解意味着我们可以将其拆分成n个监督学习问题，来解决表面上的无监督学习$p(x)$。另外，我们求监督学习问题$p(y|x)$时，也可以用传统的无监督学习策略学习联合分布$p(x,y)$，然后推断
$$
p(y|x)=\frac{p(x,y)}{\sum_{y’}p(x,y’)}
$$
尽管无监督学习和监督学习并非完全没有交集的正数概念，他们确实有助于粗略分类我们研究机器学习算法时遇到的问题。传统地，人们将回归、分类或者结构化输出问题称为监督学习。支持其他任务的密度估计通常被称为无监督学习。
学习范式的其他变种也是有可能的。例如，半监督学习中，一些样本有监督目标，但其他样本没有。在多实例学习中，样本的整个集合被标记为含有或者不含有该类的样本，但是集合中单独的样本是没有标记的。参考Kotzias(2015)了解最近深度学习模型进行多实例学习的示例。
有些机器学习算法并不是训练于一个固定的数据集上。例如，`强化学习(reinforcement learning)`算法会和环境进行交互，所以学习系统和它训练的过程会有反馈回路。这类算法超出本书范畴。参考Sutton and Barto或Bertsekas and Tsitsiklis了解强化学习相关知识，Mnih介绍了强化学习方向的深度学习方法。
大部分机器学习算法简单的训练于一个数据集上。数据集可以用很多不同方式来表示。在所有情况下，数据集都是样本的集合，而样本是特征的集合。
表示数据集常用的方法是`设计矩阵(design matrix)`。设计矩阵每一行包含一个不同的样本。每一列对应不同的特征。例如，Iris数据集包含150个样本，每个样本有4个特征。这意味着我们可以将该数据集表示为设计矩阵$\mathrm{X}\in\mathbb{R}^{150\times4}$其中$\mathrm{X}_{i,1}$表示第i个植物的萼片长度，$\mathrm{X}_{i,2}$表示第i个植物的萼片宽度等等。本书中描述的大部分学习算法都是讲述他们如何运行在设计矩阵数据集上。
当然每个样本都能表示成向量，并且这些向量维度相同，才能将一个数据集表示成设计矩阵，这一点并非永远可能。例如，你有不同宽度和高度的照片的集合，那么不同照片将会包含不同数量的像素。因此不是所有的照片都可以表示成相同长度的向量。之后章节会介绍如何处理这些不同类型的异构数据。在上述这类情况下，我们不会将数据集表示成m行的矩阵，而是表示成m个元素的结合：$\{x^{(1)},x^{(2)},...,x^{(m)}\}$。这种表示方式意味着样本向量$x^{(i)}$和$x^{(j)}$可以有不同的大小。
在监督学习中，样本包含一个标签或目标以及一组特征。例如，我们希望使用学习算法从照片中识别对象。我们需要明确哪些对象会出现在每张图片中。或许会用数字编码表示，如0表示人、1表示车、2表示猫等等。通常在处理包含观测特征的设计矩阵$\mathrm{X}$的数据集时，我们也会提供一个标签向量y，其中$y_i$表示样本i的标签。
有时标签可能不止一个数。例如，我们想要训练语音模型转录整个句子，那么每个句子样本标签是一个单词序列。
正如监督学习和无监督学习没有正式的定义，数据集或者经验也没有严格的区分。
### 5.1.4 示例：线性回归
我们将机器学习算法定义为，通过经验以提高计算机程序在某些任务上性能的算法。展示一个简单的机器学习示例：`线性回归(linear regression)`。
线性回归解决回归问题。目标是建立一个系统，将向量$x\in\mathbb{R}^n$作为输入，预测标量$y\in\mathbb{R}^n$作为输出。线性回归的输出是其输入的线性函数。令$\hat{y}$表示模型预测y应该取的值。我们定义输出为
$$
\hat{y}=w^Tx
$$
其中$w\in\mathbb{R}^n$是`参数(parameter)`向量。
参数是控制系统行为的值。$w_i$是系数，会和特征$x_i$相乘之后全部加起来。我们可以将w看作是一组决定每个特征如何影响预测的`权重(weight)`。如果特征$x_i$对应的权重$w_i$是正的，随着特征的值增加，我们的预测值$\hat{y}$也会增加。如果特征$x_i$对应权重$w_i$是负的，随着特征值的增加，我们的预测值$\hat{y}$也会减少。如果特征权重值很大，那么它对预测有很大影响。如果特征权重大小是零，那么对预测没有影响。
so我们可以定义任务T：通过输出$\hat{y}=w^Tx$从x预测y。接下来我们需要定义性能度量P。
假设我们要m个样本组成的设计矩阵，我们不用它来训练模型，而是评估模型性能如何。我们也有每个样本对应的正确值y组成的回归目标向量。因此这个数据集只是用来评估性能，我们称之为`测试集(test set)`。我们将输入的设计矩阵记作$\mathrm{X}^{(test)}$，回归目标向量记作$y^{(test)}$。
度量模型性能的一种方法是计算模型在测试集上的`均方误差(mean squared error)`。如果$\hat{y}^{(test)}$表示模型在测试集上的预测值，那么均方误差表示为：
$$
MSE_{test}=\frac{1}{2}\sum_i(\hat{y}^{(test)}-y^{(test)})^2_i
$$
直观上，当$\hat{y}^{(test)}=y^{(test)}$时，发现误差将为0。也可以看到
$$
MSE_{test}=\frac{1}{2}\sum_i\|\hat{y}^{(test)}-y^{(test)}\|^2_2
$$
所以当目标值和预测值之间的欧几里得距离增加时，误差也会增加。
为了构建机器学习算法，需要设计一个算法，通过观察训练集$(\mathrm{X}^{(train)},y^{(train)})$获取经验，减少$MSE_{test}$以改进权重w。一种直观方式是最小化训练集上的均方误差，即$MSE_{train}$。对其求导得0。
$$
\nabla_wMSE_{train}=0\\
\Rightarrow \nabla_w\frac{1}{2}\|\hat{y}^{(train)}-y^{(train)}\|^2_2=0\\
\Rightarrow \frac{1}{m}\nabla_w\|\mathrm{X}^{(train)}w-y^{(train)}\|^2_2=0\\
\Rightarrow\nabla_w(\mathrm{X}^{(train)}w-y^{(train)})^T(\mathrm{X}^{(train)}w-y^{(train)})=0\\
\Rightarrow\nabla_w(w^T\mathrm{X}^T\mathrm{X}^{(train)}w-2w^T\mathrm{X}^{(train)T}y^{(train)}+y^{(train)T}y^{(train)})=0\\
\Rightarrow 2\mathrm{X}^{(train)T}\mathrm{X}^{(train)}w-2\mathrm{X}^{(train)T}y^{(train)}=0\\
\Rightarrow w=(\mathrm{X}^{(train)T}\mathrm{X}^{(train)})^{-1}\mathrm{X}^{(train)T}y^{(train)}
$$
这个式子给出解的方程被称为`正规方程(normal equation)`。计算这个式子构成了简单的机器学习算法。下图展示了线性回归算法的使用示例。
术语`线性回归(linear regression)`通常指稍微复杂一些，附加额外参数(截距项b)的模型。在这个模型中
$$
\hat{y}=w^Tx+b
$$
从参数到预测的映射仍是一个线性函数，而从特征到预测的映射是一个仿射函数。如此扩展到仿射函数意味着模型预测的曲线仍然看起来是一条直线(知识补充：仿射函数即由 1 阶多项式构成的函数，一般形式为 $f(x)=Ax+b$，这里，A 是一个 $m\times k$ 矩阵，x 是一个 k 向量，b是一个 m 向量，实际上反映了一种从 k 维到 m 维的空间映射关系。)只是这条直线没必要经过原点。除了添加偏置参数b，我们还可以使用仅含权重的模型，但是x需要增加一项永远为1的元素。对应于额外1的权重起到了偏置参数的作用。当我们在本书中提到仿射函数时，我们会经常使用术语“线性”。
截距项b通常被称为仿射变换的`偏置(bias)`参数。这个术语的命名源自该变换的输出在没有任何输入时会偏移b。它和统计偏差中指代统计估计算法的某个量的期望估计偏离真实值的意思不一样的。
线性回归当然是一个极其简单且有局限的学习算法，但是他提供了一个说明学习算法如何工作的例子。
## 5.2 容量、过拟合和欠拟合
机器学习主要挑战是我们的算法必须能够在先前未观测的新输入上表现很好，而不只是在训练集上表现很好。在先前未观测到的输入表现很好的能力被称为`泛化(generalization)`。
当训练机器学习模型时，我们可以使用某个训练集，在训练集上计算一些被称为`训练误差(training error)`的度量误差，目标是降低训练误差。目前为止，我们讨论的是简单的优化问题。机器学习和优化不同的地方在于，我们也希望`泛化误差(generalization error)`(也被称为`测试误差(test error)`)很低。泛化误差被定义为新输入的误差期望。这里，期望的计算基于不同的可能输入。这些输入采自于系统在现实中遇到的分布。
通常，我们度量模型在训练集上分出来的`测试集(test set)`样本上的性能，来评估机器学习模型泛化误差。
在线性回归示例中，通过最小化训练误差来训练模型，
$$
\frac{1}{m^{(train)}}\|\mathrm{X}^{(train)}w-y^{(train)}\|^2_2
$$
但正在关注的是测试误差
$$
\frac{1}{m^{(train)}}\|\mathrm{X}^{(train)}w-y^{(train)}\|^2_2
$$
当我们只能观测到训练集时，我们如何才能影响测试集的效果呢？`统计学习理论(statistical learning theory)`提供了一些答案。如果训练集和测试集的数据是任意收集的，我们能做的确实有限。如果我们对训练集和测试集数据的收集方式有些假设，那么我们能够对算法做些改进。
训练集和测试集数据通过`数据生成过程(data generating process)`的概率生成分布。通常我们会做一系列被统称为`独立同分布假设(i.i.d. assumption)`的假设。该假设是说，每个数据集的样本都是彼此`相互独立(independent)`，并且训练集和测试集是`同分布的(identically distributed)`，采样自相同的分布。这个假设使我们能够在单个样本的概率分布描述数据生成过程。然后相同的分布可以用来生成每一个训练样本和每一个测试样本。我们将这个共享的潜在分布称为`数据生成分布(data generating distribution)`，记作$p_{data}$。这个概率框架和独立同分布假设允许我们从数学上研究训练误差和测试误差之间的关系。
我们能观察到训练误差和测试误差直接联系是，随机模型训练误差的期望和该模型测试误差的期望是一样的。假设我们有概率$p(x,y)$，从中重复采样生成测试集和训练集。对于某个固定的w，训练误差的期望恰好和测试误差的期望一样，这是因为两个期望的计算都使用了相同的数据集生成过程。唯一区别是数据集名字不同。
当然，当使用机器学习算法时，我们不会提前固定参数，然后采用得到两个数据集。我们采样得到训练集，然后挑选参数去降低训练集误差，然后采样得到测试集。在这个过程中，测试误差期望会大于等于训练误差期望。以下是决定机器学习算法效果是否好的因素：
1.降低训练误差。
2.缩小训练误差和测试误差的差距。
这两个因素对应机器学习的两个主要挑战：`欠拟合(underfitting)`和`过拟合(overfitting)`。欠拟合是指模型不能在训练集上获得足够低的误差。而过拟合是指训练误差和测试误差之间差距太大。
通过调整模型的`容量(capacity)`，我们可以控制模型是否偏向于过拟合或者欠拟合。模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟合训练集。容量高的模型可能会过拟合，因此记住了不适用于测试集的训练集性质。
一种控制训练算法容量的方法是选择`假设空间(hypothesis space)`，即学习算法可以选择为解决方案的函数集。广义线性回归的假设空间包含多项式函数，而非仅有线性函数。这样做就增加了模型容量。
一次多项式提供了我们已经熟悉的线性回归模型，其预测如下：
$$
\hat{y}=b+wx
$$
通过引入$x^2$作为线性回归模型的另一个特征，我们能够学习关于x的二次函数模型：
$$
\hat{y}=b+w_1x+w_2x^2
$$
尽管该模型是输入的二次函数，但输出仍然是输出的参数的线性函数。因此我们仍然可以用正规方程得到模型的闭解。(知识补充：闭式解也被称为解析解，与数值解对应。

闭式解closed form solution）也叫解析解(analytical solution)，就是一些严格的公式,给出任意的自变量就可以求出其因变量,也就是问题的解, 他人可以利用这些公式计算各自的问题。

所谓的解析解是一种包含分式、三角函数、指数、对数甚至无限级数等基本函数的解的形式。 
用来求得解析解的方法称为解析法〈analytic techniques〉，解析法即是常见的微积分技巧，例如分离变量法等。

解析解为一封闭形式〈closed-form〉的函数，因此对任一独立变量，我们皆可将其带入解析函数求得正确的相应变量。

所谓的解析解是一种包含分式、三角函数、指数、对数甚至无限级数等基本函数的解的形式。用来求得解析解的方法称为解析法。比如一元二次方程：
$$
ax^2+bx+c=0(a\neq 0)
$$
，其求解公式是：
$$
\frac{-b\pm\sqrt{b^2-4ac}}{2a}
$$
)
我们可以继续添加x的更高次幂作为额外特征，例如下面的9次多项式：
$$
\hat{y}=b+\sum_{i=1}^9w_ix^i
$$
当机器学习算法的容量适合于所执行任务的复杂度和所提供训练数据的数量时，算法效果通常会最佳。容量不足的模型不能解决复杂任务。容量高的模型能够解决复杂的任务，但是当其容量高于任务所需时，可能会过拟合。
下图展示了这个原理使用情况，比较了线性，二次和9次预测器拟合真实二次函数的效果。线性函数无法刻画真实函数的曲率，so欠拟合。9次函数能够表示正确的函数，但是训练参数比训练样本还要多，so它能够表示无限多个刚好穿越训练样本点的很多其他函数。不太可能从很多不同的解中选出一个泛化良好的。这个问题中，二次模型非常符合真实结构，so可以很好的泛化到新数据集上。
探讨了通过改变输入特征的数目和加入这些特征对应的参数，改变模型的容量。还有很多方法能够改变模型容量。容量不仅取决于模型的选择。模型规定了调整参数降低训练目标时，学习算法可以从哪些函数族中选择函数。这被称为模型的`表示容量(representational capacity)`。在很多情况下，从这些函数中挑选出最优函数是非常困难的优化问题。实际中，学习算法不会找到最优函数，而仅是找到一个可以大大降低训练误差的函数。额外的限制因素，比如优化算法的不完美，意味着学习算法的`有效容量(effective capacity)`可能小于模型族的表示容量。
提高机器学习模型泛化的现代思想可以追溯到早在托勒密时期的哲学家的思想。许多早期的学者提出一个简约原则，现在广泛被称为`奥卡姆剃刀(Occam’s razor)`。该原则指出，在同样能够解释已知观测现象的假设中，我们应该挑选“最简单”的那一个。此想法在20世纪，由统计学系理论创始人形式化并精确化的。
统计学习理论提供了量化模型容量的不同方法。在这些中，最有名的是Vapnik-Chervonenkis 维度，VC。(知识补充：传统的定义是：对一个指示函数集，如果存在H个样本能够被函数集中的函数按所有可能的2的H次方种形式分开，则称函数集能够把H个样本打散；函数集的VC维就是它能打散的最大样本数目H。若对任意数目的样本都有函数能将它们打散，则函数集的VC维是无穷大，有界实函数的VC维可以通过用一定的阈值将它转化成指示函数来定义。
VC维反映了函数集的学习能力，VC维越大则学习机器越复杂（容量越大），遗憾的是，尚没有通用的关于任意函数集VC维计算的理论。

对于二维线性函数集合，3个点只要能找到一种摆放方式能被正确分类，而4个点无论怎么摆都无法正确分类，那么二维线性函数集合的VC维是3)
VC维，度量二元分类器的容量。VC维定义为该分类器能够分类的训练样本的最大数目。假设存在m个不同x点的训练集。分类器可以任意标记该m个点，VC维被定义为m的最大可能值。
量化模型的容量使得统计学习理论可以进行量化预测。统计学习理论中最重要的结论阐述了训练误差和泛化误差之间差异的上界随着随着模型容量增长而增长。但随着训练样本增多而下降。这些边界为机器学习算法可以有效解决问题提供了理论验证，但是他们很少应用于实际中的深度学习算法。一部分原因是边界太松，另一部分原因是很难确定深度学习算法的容量。由于有效容量受限于优化算法能力，确定深度学习模型容量的问题特别困难。而且对于深度学习中的一般非凸优化问题，我们只有很少的理论分析。
虽然更简单的函数更可能泛化(训练误差和测试误差的差距小)，但我们仍然需要选择一个充分复杂的假设以达到低的训练误差。通常当模型容量上升时，训练误差会下降，直到其渐进最小可能误差。泛化误差通常是一个关于模型容量U形曲线函数。如图。

为考虑容量任意高的极限情况，我们介绍`非参数(non- parametric)`模型的概念。至此我们只探讨过参数模型，例如线性回归。参数模型学习的函数在观测到新数据前，参数向量的分量个数是有限且固定的。非参数模型没有这些限制。
有时非参数模型仅是一些不能实际实现的理论抽象(比如搜索所有可能概率分布的算法)。然而，也可以设计一些实用的非参数模型，使他们的复杂度和训练集大小有关。这种算法的一个示例是`最近邻回归(nearest neighhor regression)`。不像线性回归有固定长度的向量作为权重，最近邻回归模型存储了训练集中所有的X和y。当需要为测试点x分类时，模型会查询训练集中离该点最近的点，并返回相关的回归目标(个人理解就是类别标签)。换言之，$\hat{y}=y_i$其中$i=arg\min\|\mathrm{X}_{i,:}-x\|_2^2$。在最近向量不唯一的情况下，如果允许算法对所有离x最近的$\mathrm{X_{i,:}$关联的$y_i$求平均，那么该算法会在任意回归数据集上达到最小可能的训练误差。
也可以将参数学习算法嵌入另一个增加参数数目的算法来创建非参数学习算法。例如，可以想象这样的算法，外层循环调整多项式的次数，内层循环通过线性回归学习模型。
理想模型假设我们预先知道生成数据真实概率分布。然后这样的模型仍然会在很多问题上发生错误，因为分布中仍然会有一些噪声。在监督学习中从x到y的映射可能内在是随机的，或者y可能是其他变量的确定性函数。从预先知道的真实分布$p(x,y)$预测而出现的误差被称为`贝叶斯误差(Bayes error)`。(知识补充：贝叶斯误差（bayes error rate)是指在现有特征集上，任意可以基于特征输入进行随机输出的分类器所能达到最小误差。也可以叫做最小误差。

先直接回答题主的疑问：“书上说是从预先知道的分布预测而出现的c误差，既然已经预先知道分布了，那么为什么还有误差呢？”

回答：分布是真实的，但预测的输出只能是一个值，所以会有误差。例如，假设真实世界中90%长头发的人为女性，10%为男性（这是已知的真实分布）；此时已知一个人头发长，预测该同学性别。由于只能预测男/女。此时即使你知道真实分布，预测为女，也会有10%的误差。这就是贝叶斯误差。

比较重要的是理解分布是真实的，但输出只能是一个值，所以会有误差。如果输出也可以是分布，那就没有误差了。)
训练误差和泛化误差会随训练集的大小发生变化。泛化误差的期望从不会因训练样本数目的增加而增加。对于非参数模型而言，更多的数据会得到更好的泛化能力，直到达到最佳可能的泛化误差。任何模型容量小于最优容量的固定参数模型会渐进到大于贝叶斯误差的误差值。值得注意的是，具有最优容量的模型仍然有可能在训练误差和泛化误差之间存在很大的差距。这种情况下可以通过收集更多的训练样本来缩小差距。
### 5.2.1 没有免费的午餐定理
学习理论表明机器学习算法能够在有限个训练集样本中很好的泛化。这似违背一些基本的逻辑原则。归纳推理，或是从一组有限的样本中推断一般的规则，在逻辑上不是有效的。为了逻辑的推断一个规则去描述集合中的元素，我们必须有集合中每个元素的信息。
在一定程度上，机器学习仅通过概率法则就可以避免这个问题，而无需使用纯逻辑推理整个确定性法则。机器学习保证找到一个在所关注的大多数样本上可能正确的规则。可惜，即使这样也不能解决整个问题。机器学习`没有免费午餐定理(no free lunch theorem)`表明，在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上没有一个机器学习算法总是比其他的要好。我们能够设想的最先进的算法和简单得将所有点归为同一类简单算法有着相同的平均性能(在所有可能的任务上)。
幸运的是，这些结论仅在我们考虑所有可能的数据生成分布时才成立。在真实世界应用中，如果我们对遇到的概率分布进行假设的话，那么我们可以设计在这些分布上效果良好的学习算法。
这意味着机器学习研究的目标不是找一个通用的学习算法或是绝对最好的的学习算法。反之我们的目标是理解什么样的分布与人工智能获取经验的“真实世界”相关，什么样的学习算法在我们关注的数据生成分布上效果最好。
### 5.2.2 正则化
没用免费午餐定理暗示我们必须在特定任务上设计良好的机器学习算法。我们建立一组学习算法的偏好来达到这个要求。当这些偏好和我们希望算法解决的学习问题相吻合时，性能会更好。
至此，我们具体讨论修改学习算法的方法只有他，通过增加或减少学习算法可选假设空间的函数来增加或减少模型的表示容量。我们列举一个具体示例是线性回归增加或减少多项式的次数。目前为止讨论的问题都是过度简单化的。
算法的效果不仅很大程度上受影响于假设空间的函数数量，也取决于这些函数的具体形式。我们已经讨论的学习算法(线性回归)具有包含其输入的线性函数集的假设空间。对于输入和输出确实接近线性相关的问题，这些线性函数是很有用的。对于完全非线性的问题它们不要有效。例如，用线性回归，从x预测$\sin(x)$效果不好。因此可以通过两种方式控制算法的性能，一是允许使用的函数种类，二是这些函数的数量。
在假设空间中，相比于某一个学习算法，我们可能更偏好另一个学习算法。这意味着两个函数都是符合条件的，但是我们更偏好其中一个。只有非偏好函数比偏好函数在训练数据集上效果明显好很多时，我们才会考虑非偏好函数。
例如，我们可以加入`权重衰减(weight decay)`来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则相的和$J(x)$，其偏好于平方$L^2$范数较小的权重。具体如下：
$$
J(x)=MSE_{train}+\lambda w^Tw
$$
其中$\lambda$是提前挑选的值，控制我们偏好小范数权重的程度。当$\lambda=0$时，我们没有任何偏好。越大的$\lambda$偏好范数越小的权重。最小化$J(x)$可以看作是拟合训练数据和偏好小权重范数之间的权衡。这会使得解决方案斜率较小，或是将权重放在较少的特征上(个人理解，偏好小w，是选值较小的w)。我们可以训练具有不同$\lambda$值的高次多项式回归模型，来举例说明如何通过权重衰减控制模型欠拟合或过拟合趋势。如下图。

更一般的，正则化一个学习函数$f(x;\theta)$的模型。我们可以给代价函数添加被称为`项正则化(regulaizer)`的惩罚。在权重衰减的例子中，正则化项是$\Omega(\omega)=\omega^T\omega$。第七章看到更多的正则化项。
表示对函数的偏好是比增减假设空间的成员更一般的控制模型容量的方法。我们可以将去掉假设空间中的某个函数看作是对不赞成这个函数的无限偏好。
中我们的权重衰减示例中，通过在最小化的目标中额外增加一项，我们明确的表示了偏好权重较小的线性函数。有很多其他方法隐式或显式地表示对不同解的偏好。总而言之，这些方法都被称为`正则化(regularization)`。正则化是指我们修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相媲。
没用免费的午餐定理已经清楚的阐述了没有最优的学习算法，没有最优的正则化形式。我们必须挑选一个非常适合我们所要解决的任务的正则形式。深度学习中普遍的理念是大量任务也许都可以使用非常通用的正则化形式来有效解决。
## 5.3 超参数和验证集
大多数机器学习算法都有超参数，可以设置来控制算法行为。超参数的值不是通过学习算法本身学习出来的(尽管可以设计嵌套学习过程，一个学习算法为另一个学习算法学出最优超参数)。
之前图所示多项式回归示例中，有一个超参数：多项式的次数，作为容量超参数。控制权重衰减程度的$\lambda$是另一个超参数。
有时一个选项被设为学习算法不用学习的超参数，是因为它太难优化了。更多的情况是，该选项必须是超参数，因为它不适合在训练集上学习。这适用于控制模型容量的所有超参数。如果在训练集上学习超参数，这些超参数总是趋向于最大可能的模型容量，导致过拟合。例如，相比低次多项式和正的权重衰减设定，更高次的多项式和权重衰减参数设定$\lambda=0$总能在训练集上更好的拟合。
为解决这个问题，我们需要一个训练算法观测不到的`验证集(validation set)`样本。
之前讨论过和训练数据相同分布样本组成的测试集，它可以用来评估学习过程完成后的学习器的泛化误差。其重点在于测试样本不能以任何形式参与到模型的选择中，包括设定超参数。基于这个原因，测试集的样本不能用于验证集。因此我们总是从训练数据中构建验证集。将训练数据分成两个不相交的子集。其中一个用于学习参数，另一个作为验证集，用于评估训练中或训练后的泛化误差，更新超参数。用于学习参数的数据子集通常被称为训练集，尽管这会和整个训练过程用到的更大的数据集相混。用于挑选超参数的数据子集被称为`验证集(validation set)`。通常80%数据用于训练，20%数据用于验证。由于验证集是用来“训练”超参数的，尽管验证集的误差通常会比训练集的误差小，验证集会低估泛化误差。所以超参数优化完成之后，泛化误差可能会通过测试集评估。
在实际中，当相同的测试集已在很多年中重复地用于评估不同算法的性能，并考虑学术界在该测试集上的各种尝试，我们最后可能也会对测试集有着乐观的估计。基准会因之变得陈旧，而不能反映系统的真实性能。值得庆幸的是，学术界往往会移到新的(通常会更巨大，更具挑战性)基准数据集上。
### 5.3.1 交叉验证
将数据集分成固定的训练集和固定的测试集后，若测试集的误差很小，这将是有问题的。一个小规模的测试集意味着平均测试误差估计的统计不确定性，使得很难判断算法A是否比算法B在给定的任务上做得更好。
当数据集有十万计或者更多的样本时，这不会是一个验证问题，当数据集太小时。也有替代方法允许我们使用所有的样本估计平均测试误差，代价是增加了计算量。这些过程是基于在原始数据上随机采样或分离出不同数据集上重复训练和测试的想法。最常见的是k-折交叉验证过程，如下算法，将数据集分成k个不重合的子集。测试误差可以估计为k次计算后的平均测试误差。在第i次测试时，数据的第i个子集用于测试集，其他数据应于训练集。带了一个问题是不存在平均误差方差的无偏估计，(Bengio and Grandvalet, 2004)通常使用近似来解决。
***
算法 k-折交叉验证算法。当给定数据集$\mathbb{D}$对于简单的训练/测试或训练/验证分割而言太难以产生泛化误差的准确估计时(因为在小的测试集上L可能具有过高的方差)，k-折交叉验证算法可以用于估计学习算法A的泛化误差。数据集$\mathbb{D}$包含的元素是抽象样本$z^{(i)}$(对于第i个样本)，在监督学习情况代表(输入，目标)对$z^{(i)}=(x^{(i)},y^{(i)})$，或者无监督学习的情况下仅用于输入$z^{(i)}=x^{(i)}$。该算法返回$\mathbb{D}$中每个示例的偏差向量e，其均值是估计的泛化误差。单个样本上的误差可用于计算平均值周围的置信区间。虽然这些置信区间在使用交叉验证之后不能很好的证明，但通常的做法是只有当算法A误差的置信区间低于并且不与算法B置信区间相交时，我们才声明算法A比算法B好。
***
**Define** KFoildXV($\mathbb{D}$,A,L,K):
**Require**: $\mathbb{D}$为给定数据集，其中元素为$z^{(i)}$
**Require**:A为学习算法，可视为一个函数(使用数据集作为输入，输出一个学好的函数)
**Require**:L为损失函数，可视为来自学好的函数f，将样本$z^{(i)}\in\mathbb{D}$映射到$\mathbb{R}$中标量的函数
**Require**:k为折数
  将$\mathbb{D}$分为k个互斥子集$\mathbb{D}_i$，它们的并集为$\mathbb{D}$
&nbsp;&nbsp;for i from 1 to k do
&nbsp;&nbsp; &nbsp; &nbsp; $f_i=A(\mathbb{D}$\\ $\mathbb{D}_i)$
&nbsp;&nbsp; &nbsp; &nbsp; for $z^{(j)}$ in $\mathbb{D}_i$ do
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; $e_j=L(f_i,z^{(j)})$
&nbsp;&nbsp; &nbsp; &nbsp; end for
&nbsp;&nbsp; end for
&nbsp;&nbsp; Return e
***
## 5.4 估计、偏差和方差
统计领域为我们提供很多工具来实现机器学习目标不仅可以解决训练集上的任务，还可以泛化。基本概念，例如，参数估计、偏差和方差对于正式的刻画泛化、欠拟合和过拟合都非常有用。
### 5.4.1 点估计
点估计试图为一些感兴趣的量提供单个“最优”预测。一般的，感兴趣的量可以是单个参数，或是某些参数模型中的一个向量参数。例如线性规划中的权重，也有可能是整个函数。